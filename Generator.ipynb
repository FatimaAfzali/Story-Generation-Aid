{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from pattern.en import conjugate, lemma\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) \n",
    "\n",
    "#get a dictionary mapping words to their parts of speech; get the keys separately, for faster lookup times\n",
    "brown = {v[0]: v[1] for v in nltk.corpus.brown.tagged_words()}\n",
    "brown_keys = brown.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we are going to have 3 classes\n",
    "#Embedding provides tools for the manipulation/analysis of word embeddings\n",
    "#Grammar provides tools for the application of formal grammars to word embeddings\n",
    "#Grammatical provides a wrapper for the two to interact\n",
    "class Embedding:\n",
    "    def __init__(self, embeddings, numbers, basis = \"I\"): \n",
    "        self.vectors = np.array(embeddings)\n",
    "        self.dims = self.vectors.shape #(number of vectors, dimensionality of each)\n",
    "        self.word2int = numbers\n",
    "        self.int2word = {v: k for k, v in numbers.items()} #in case we want a reverse lookup\n",
    "        self.important = None\n",
    "        self.basis = basis #basis for the embedding vectors; can be changed, preserved across classes\n",
    "        if self.basis == \"I\": \n",
    "            self.basis = np.eye(self.dims[1]) #initialize to identity of right size (impossible to specify exact size in default argument, need to check dimensionality of embeddings)\n",
    "    \n",
    "    def get_vect(self, pos): \n",
    "        #Get the vector for either the input word or the input index\n",
    "        if type(pos) == str: \n",
    "            return self.vectors[self.word2int[pos]]\n",
    "        if type(pos) == int: \n",
    "            return self.vectors[pos]\n",
    "        return\n",
    "    \n",
    "    def get_word(self, n): \n",
    "        #Get the word for the input index; if given a list of input indices, return corresponding list of words\n",
    "        if type(n) == list: \n",
    "            return map(self.get_word, n)\n",
    "        return self.int2word[n]\n",
    "    \n",
    "    def filter_by_words(self, words, perm = False): \n",
    "        #returns list of indices corresponding to the input words (i'm not sure this is useful, but don't want to delete it yet)\n",
    "        #the perm argument will show up repeatedly — generally, perm = False means return just the list of indices\n",
    "        #while perm = True means filter this class's words/vectors to just those indices (returns void)\n",
    "        L = []\n",
    "        for i in range(self.dims[0]): \n",
    "            if self.get_word(i) in words: \n",
    "                L.append(i)\n",
    "        if not perm: \n",
    "            return L\n",
    "        else: \n",
    "            #the form class function will either overwrite this class or form a new one and return that, depending on whether perm was True or False, respectively\n",
    "            self.form_class(L, True)    \n",
    "            \n",
    "    def filter_by_dot(self, words, angle = 90, normalize = True, prop = None, perm = False): \n",
    "        #filter by dot product relative to selected words (letting N be the number of words in the wordlist, return the prop * N best words)\n",
    "        #since the formulae for affine hyperplane selection and angle selection are the same save for normalization, \n",
    "        #normalization = True sorts by angle (maximal angle allowable is the angle argument), and normalization = False sorts by affine hyperplane\n",
    "        #if multiple words are given, any given word must be good with *all* of them\n",
    "        \n",
    "        if type(words) == str: words = [words]\n",
    "        nice = np.cos(angle / 180.0 * np.pi) #degrees -> radians -> dot product (since it's cos(angle) that's proportional to the dot product)\n",
    "        V = np.matrix(self.vectors) #N x D matrix, where N is number of vectors and D is dimensionality\n",
    "        w = np.matrix([self.get_vect(word) for word in words]) #L x D matrix, where L is the number of words in the words argument\n",
    "        #multiplying w by the transpose of V will result in an L x N matrix, \n",
    "        #where the values in the ith column will correspond to the dot products of the ith vector with the vectors of all the words in the words argument\n",
    "        #so the minimal value in the ith column will correspond to the ith word's worst match\n",
    "        #(and if this match is less than nice, we deny that word entry into the new list)\n",
    "        good = []\n",
    "        if normalize: \n",
    "            M = np.array(((w * V.T / np.linalg.norm(V, axis = 1)).T  / np.linalg.norm(w, axis = 1)).T) #if normalize, normalize by norms of vectors, so as to get angle\n",
    "        else: \n",
    "            M = np.array(w * V.T) #otherwise, affine hyperplane\n",
    "        if prop != None: \n",
    "            M = M.min(axis = 0) #get minimum along columns, as previously stated\n",
    "            #now, we want the (prop * N) best words, so we sort the entire wordlist by their values in M, get the first prop * N, then sort *those* to get an ordered list of the best words\n",
    "            good = sorted(sorted(range(self.dims[0]), key = lambda x: M[x])[:int(self.dims[0] * prop)])\n",
    "        else: \n",
    "            #if prop was not specified, just return those that are above the threshold (but it's *really* hard to control the size of this list, so using prop is way better)\n",
    "            for i, k in enumerate(M.min(axis = 0)): \n",
    "                if k > nice: \n",
    "                    good.append(i)\n",
    "        if not perm: \n",
    "            return good\n",
    "        else: \n",
    "            self.form_class(good, True)\n",
    "\n",
    "    def filter_by_norm2(self, words, weights = None, prop = 0.5, order = 2, power = 1, power2 = 3, resort = True, perm = False): \n",
    "        return 'fuck'\n",
    "        #sort words in wordlist by how close they are to words in words argument, with weights allowing you to control which words in the words argument are most important\n",
    "        #see above note for details on how this works\n",
    "        if weights == None: \n",
    "            weights = [1 for x in words]\n",
    "        func = lambda x: sum([((weights[i] * np.linalg.norm(self.get_vect(words[i]) - self.get_vect(x), ord = order)**power2))**power for i in range(len(words))])\n",
    "        good = sorted(range(self.dims[0]), key = func)[:int(self.dims[0] * prop)]\n",
    "        if resort: \n",
    "            good = sorted(good)\n",
    "        if not perm: \n",
    "            return good\n",
    "        else: \n",
    "            self.form_class(good, True)\n",
    "    \n",
    "    def filter_by_norm(self, words, weights = None, prop = 0.5, order = 2, power = 1, power2 = 1, resort = True, perm = False): \n",
    "        if weights == None:\n",
    "            weights = [1 for x in words]\n",
    "        M = [0 for x in range(self.dims[0])]\n",
    "        for x in range(self.dims[0]): \n",
    "            k = self.get_vect(x)\n",
    "            for i in range(len(words)): \n",
    "                v = self.get_vect(words[i])\n",
    "                M[x] += weights[i] * np.linalg.norm(v - k, ord = order) ** power2\n",
    "        if power != 1: \n",
    "            M = [M[x] ** power for x in range(self.dims[0])]\n",
    "        good = sorted(range(self.dims[0]), key = lambda x: M[x])[:int(self.dims[0] * prop)]\n",
    "        if resort: \n",
    "            good = sorted(good)\n",
    "        if not perm: \n",
    "            return good\n",
    "        else: \n",
    "            self.form_class(good, True)\n",
    "            \n",
    "    def match(self, vect): \n",
    "        #what word is closest to a given vector? \n",
    "        #e.g., match(get_vect(\"queen\") + get_vect(\"man\")) might return \"king\", but idk since this is an arbitrary example \n",
    "        ind = None\n",
    "        val = np.inf\n",
    "        for i in range(self.dims[0]): \n",
    "            val2 = np.linalg.norm(vect - self.get_vect(i))\n",
    "            if val2 < val and val2 != 0: \n",
    "                val = val2\n",
    "                ind = i\n",
    "        return ind\n",
    "    \n",
    "    def change_basis(self, basis): \n",
    "        #given a new basis, change the basis of the matrix\n",
    "        #you don't need this\n",
    "        V = np.matrix(self.vectors)\n",
    "        V = V * basis\n",
    "        self.vectors = np.array(V)\n",
    "        self.basis = self.basis * basis\n",
    "        \n",
    "    def to_identity(self): \n",
    "        #Revert to the identity basis using the Moore-Penrose inverse\n",
    "        self.change_basis(np.linalg.pinv(self.basis))\n",
    "        \n",
    "    def realign_PCA(self): \n",
    "        #perform a principal component analysis on the matrix of vectors (V), \n",
    "        #finding those vectors that best explain the variance among word embeddings \n",
    "        #geometric intuition: https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/1200px-GaussianScatterPCA.svg.png\n",
    "        #(note: these vectors are canonically the eigenvectors of the covariance matrix, which itself is proportional to the transpose of V times V!)\n",
    "        #then change the basis so as to make those vectors the principal axes\n",
    "        #again, you don't need this, but really cool\n",
    "        V = np.matrix(self.vectors)\n",
    "        W = V.T * V\n",
    "        Eval, Evec = np.linalg.eig(W) #PCA\n",
    "        self.vectors = np.array(V * Evec) #change of basis \n",
    "        self.basis = self.basis * Evec\n",
    "        \n",
    "    def project2d(self, title = None): \n",
    "        #who cares about data visualization? \n",
    "        #not me!\n",
    "        #if it's under 6 dimensions, i don't care\n",
    "        #jk\n",
    "        #but this is still pretty useless except for playing around with new sets of embeddings\n",
    "        b = Embedding(self.vectors, self.word2int, self.basis)\n",
    "        B = np.diag([1, 1] + [0] * (self.dims[1] - 2))[0:2].T\n",
    "        b.realign_PCA() #may as well make the two dimensions useful ones\n",
    "        \n",
    "        b.change_basis(B) # :(\n",
    "        fig, ax = plt.subplots()\n",
    "        for word, x, y in zip(b.get_word(range(b.dims[0])), b.vectors.T[0], b.vectors.T[1]): \n",
    "            ax.annotate(word, (x, y))\n",
    "        plt.plot([-10, 10], [0, 0], 'k--')\n",
    "        plt.plot([0, 0], [-10, 10], 'k--')\n",
    "        plt.xlim(min(b.vectors.T[0]), max(b.vectors.T[0]))\n",
    "        plt.ylim(min(b.vectors.T[1]), max(b.vectors.T[1]))\n",
    "        if title != None: \n",
    "            plt.title(title)\n",
    "        plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "        plt.show()\n",
    "        #plt.plot(b.vectors.T[0], b.vectors.T[1], '+')\n",
    "        \n",
    "    def strip_pos(self, acceptable = [\"NN\", \"NNS\", \"RB\", \"RBR\", \"RBS\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBZ\", \"VBP\", \"JJ\"], perm = False): \n",
    "        #strips words that don't carry useful meanings, like \"while\" and \"should\" and \"cannot\" and etc.\n",
    "        #the acceptable default set comes from the list of POS tags NLTK uses: the default arg references nouns, adverbs, adjectives, and verbs\n",
    "        L = []\n",
    "        for w, i in self.word2int.items(): \n",
    "            try: \n",
    "                if brown[w] in acceptable: \n",
    "                    L.append(i)\n",
    "            except: \n",
    "                pass \n",
    "        if perm: \n",
    "            self.form_class(L, perm = True)\n",
    "        else: \n",
    "            return L\n",
    "        \n",
    "    def form_class(self, L, perm = False): \n",
    "        #as stated before, let L be a list of indices\n",
    "        #then form_class returns a class whose words/vectors are those specified by L\n",
    "        #and if perm (short for permanent) = True, form_class overwrites *this* class with that class, by recalling self.__init__\n",
    "        ind = 0\n",
    "        vec = []\n",
    "        w2i = {}\n",
    "        for i in range(self.dims[0]): \n",
    "            if i in L: \n",
    "                vec.append(self.get_vect(i))\n",
    "                w2i[self.get_word(i)] = ind\n",
    "                ind += 1\n",
    "        if not perm: \n",
    "            return Embedding(vec, w2i, self.basis)\n",
    "        else: \n",
    "            self.__init__(vec, w2i, self.basis)\n",
    "            \n",
    "    def select_random(self, n = 1): \n",
    "        #returns either a single random word from an Embedding instantiation's wordlist, or returns a list of random words, depending on n\n",
    "        if n == 1:\n",
    "            return random.choice(self.word2int.keys())\n",
    "        L = []\n",
    "        i = 0\n",
    "        if n > self.dims[0]: \n",
    "            return \"fuck\"\n",
    "        while i < n: \n",
    "            a = random.choice(self.word2int.keys())\n",
    "            if a not in L: \n",
    "                L.append(a)\n",
    "            i += 1\n",
    "        return L\n",
    "    \n",
    "    def sample(self, words, weights, prop, n = 20):\n",
    "        #again, useless except for playing around with different word embeddings\n",
    "        B = self.form_class(self.filter_by_norm(words, weights, prop))\n",
    "        B.project2d(' '.join(words))\n",
    "        print ' '.join(words) + ': ' + ' '.join(B.select_random(n))\n",
    "        \n",
    "    def copy(self): \n",
    "        return Embedding(self.vectors, self.word2int, self.basis)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grammar: \n",
    "    def __init__(self, productions, terminals, pos_dict): \n",
    "        #Every grammar needs a set of productions from which to build abstract structures, \n",
    "        #and a set of terminals with which to fill the final structures in with.\n",
    "        #self.pos_dict will let us convert between NLTK parts of speech and the terminals' own parts of speech \n",
    "        #(e.g., it will tell us that \"VBG\" and \"VBN\" correspond to \"<verb>\", \"JJ\" to \"<adjective>\", etc.)\n",
    "        self.prods = productions\n",
    "        self.terms = terminals\n",
    "        self.pos_dict = pos_dict\n",
    "        self.embedding = ref_embedding\n",
    "        \n",
    "    def clean(self, string): \n",
    "        #gets a list of words that were just filled in from an abstract structure, and tidies it up\n",
    "        #e.g., [\"A\", \"astronaut\", \"land\", \"on\", \"the\", \"moon\"] becomes \"An astronaut landed on the moon.\"\n",
    "        for i in range(len(string)): \n",
    "            #first, make all verbs past tense\n",
    "            try: \n",
    "                if self.pos_dict[brown[string[i]]] == '<verb>': \n",
    "                    string[i] = conjugate(string[i], tense = 'past')\n",
    "            except KeyError: \n",
    "                pass\n",
    "        if type(string) == list: \n",
    "            #then, convert list to space-separated string\n",
    "            string = ' '.join(string)\n",
    "        for i in ';,.!': \n",
    "            #then, remove spaces directly before punctuation (e.g., \"Go !\" -> \"Go!\")\n",
    "            string = string.replace(' ' + i, i)\n",
    "        for i in range(len(string)): \n",
    "            #then, capitalize all initial words in sentences\n",
    "            if i==0 or (i > 2 and string[i-2] in '.!\\n'): \n",
    "                string = string[:i] + string[i].upper() + string[i+1:]\n",
    "            #then, replace \"a\" with \"an\" when necessary\n",
    "            if string[i]=='a' and (i > 1 and string[i-1]==string[i+1]==' ') and (len(string) > i+2 and string[i+2] in 'aeiou'): \n",
    "                string = string[:i] + 'an' + string[i+1:]\n",
    "        return string\n",
    "    \n",
    "    def fill(self, L): \n",
    "        #Naively fill in the abstract sentence structure with words\n",
    "        #Just call the terminals corresponding to the given part of speech in the structure, and pick a random one\n",
    "        L2 = [random.choice(self.terms[i].split('|')) for i in L]\n",
    "        return L2\n",
    "    \n",
    "    def smart_fill(self, L, attn_span = 3, variety = 0.1, attn = None): \n",
    "        #Fill in the abstract sentence structure with words\n",
    "        #Keep an \"attention\" list (attn) which determines the words used to fill in the structure, \n",
    "        #said words then being added to the attention list themselves, while old words are removed from the attention list \n",
    "        #Start off the attention list with the word corresponding to the subject\n",
    "        if attn == None: attn = []\n",
    "        attn = [self.terms['<subject>']] + attn\n",
    "        L2 = []\n",
    "        i = 0\n",
    "        for i in L: \n",
    "            if i in ['<verb>', '<adjective>', '<noun>']: \n",
    "                #If the part of speech we're filling in has semantic content (it's one of those POSes), \n",
    "                #use the attention list to construct a list of related words, filter those to get the ones with the right POS, then pick one of those\n",
    "                possibles = [x for x in self.embedding.get_word(self.embedding.filter_by_norm(attn, range(1, attn_span+1)[::-1], prop = variety))] #range(1, attn_span+1)[::-1]\n",
    "                possibles2 = []\n",
    "                for j in possibles: \n",
    "                    if brown[j] in self.pos_dict.keys() and self.pos_dict[brown[j]] == i: \n",
    "                        possibles2.append(j)\n",
    "                possibles = possibles2\n",
    "                word = random.choice(possibles)\n",
    "                while word in L2: \n",
    "                    word = random.choice(possibles)\n",
    "                L2.append(word)\n",
    "                attn.append(word)\n",
    "                if len(attn) > attn_span: \n",
    "                    #attn = attn[1:]\n",
    "                    attn.remove(random.choice(attn))\n",
    "                #print attn\n",
    "                    \n",
    "            else: \n",
    "                word = random.choice(self.terms[i].split('|'))\n",
    "                L2.append(word)\n",
    "        return L2\n",
    "    \n",
    "    def parse(self, general = False, start = '<sentence>', lim = 20, attn_span = 3, variety = 0.1, attn = None): \n",
    "        #Use the productions to turn the starting structure into a structure ready to be converted into terminals\n",
    "        #Recursively, while there's a tag in the start variable that's not in the list of terminal tags, \n",
    "        #replace all the start tags with new sets of tags based on the production rules\n",
    "        #The lim argument sets an upper bound to recursion depth — if it's exceeded, we assume we're on a runaway train, and start from scratch\n",
    "        #Then, unless general is True, we send that final structure to be filled in with smart_fill, \n",
    "        #passing our attn_span and variety arguments to that\n",
    "        if lim == 0: \n",
    "            return self.parse(general, '<sentence>', lim, attn_span = attn_span, variety = variety, attn = attn)\n",
    "        start = random.choice(start.split('|')).split()\n",
    "        start2 = []\n",
    "        give = False\n",
    "        for i in range(len(start)): \n",
    "            if start[i] in self.prods.keys(): \n",
    "                start2 += random.choice(self.prods[start[i]].split('|')).split()\n",
    "                give = True\n",
    "            else: \n",
    "                start2 += [start[i]]\n",
    "        if give and (lim != 0): \n",
    "            return self.parse(general, ' '.join(start2), lim-1, attn_span = attn_span, variety = variety, attn = attn)\n",
    "        else: \n",
    "            if general: \n",
    "                return start2\n",
    "            else: \n",
    "                return self.smart_fill(start2, attn_span, variety, attn)\n",
    "    \n",
    "    def generate(self, start = '<sentence>', lim = 20, attn_span = 3, variety = 0.1, attn = None): \n",
    "        #Simple wrapper for parsing and cleaning a sentence — this will produce full, clean descriptions on its own\n",
    "        return self.clean(self.parse(False, start, lim, attn_span, variety, attn))\n",
    "        \n",
    "    def add_terminal(self, term, pos): \n",
    "        #Add a terminal to the list of terminals, given the part of speech to add it to\n",
    "        if pos == '<none>': \n",
    "            return\n",
    "        if ' ' in pos: \n",
    "            for i in pos.split(): \n",
    "                self.add_terminal(term, i)\n",
    "            return\n",
    "        if pos == '<verb>': \n",
    "            term = conjugate(term, tense = 'past')\n",
    "        if self.terms[pos] == '': \n",
    "            self.terms[pos] = term\n",
    "        else: \n",
    "            self.terms[pos] += '|' + term\n",
    "            \n",
    "    def add_terminal_auto(self, term): \n",
    "        #Add a terminal to the list of terminals, automatically determining the part of speech to add it to\n",
    "        #Determine it via self.pos_dict and the brown dict\n",
    "        #And if we get a *list* of terms, add all of them automatically (very convenient!)\n",
    "        if type(term) == list: \n",
    "            map(self.add_terminal_auto, term)\n",
    "            return\n",
    "        try: \n",
    "            pos = brown[term]\n",
    "            self.add_terminal(term, self.pos_dict[pos])\n",
    "        except KeyError: \n",
    "            return\n",
    "            \n",
    "    def clear_terminals(self, pos): \n",
    "        #Clear a part of speech, to start anew for whatever reason\n",
    "        self.terms[pos] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper(): \n",
    "    #A class for entities such as characters, objects, and locations. \n",
    "    def __init__(self, embedding, grammar): \n",
    "        self.embedding = embedding\n",
    "        self.grammar = grammar\n",
    "        self.grammar.embedding = embedding\n",
    "        self.rom = [] #Permanent characteristics\n",
    "        self.ram = [] #Semi-permanent characteristics\n",
    "        self.cache = []\n",
    "        self.cache_size = 8\n",
    "        self.def_proportion = 200.0 / self.embedding.dims[0]\n",
    "        self.previous = None\n",
    "        self.height = 0\n",
    "        self.next = []\n",
    "    \n",
    "    def increase_height(self): \n",
    "        self.height += 1\n",
    "        if self.previous != None: \n",
    "            self.previous.increase_height()\n",
    "            \n",
    "    def base(self, term): \n",
    "        if type(term) == list:\n",
    "            map(self.base, term)\n",
    "            return\n",
    "        self.rom.append(term)\n",
    "        \n",
    "    def queue(self, term, method = \"lifo\"): \n",
    "        if type(term) == list: \n",
    "            map(lambda x: self.queue(x, method), term)\n",
    "            return\n",
    "        if len(self.cache) < self.cache_size: \n",
    "            self.cache.append(term)\n",
    "            return\n",
    "        if method == \"fifo\": \n",
    "            self.cache[self.cache - 1] = term\n",
    "        if method == \"lifo\": \n",
    "            self.cache = self.cache[1:]\n",
    "            self.cache.append(term)\n",
    "        if method == \"random\": \n",
    "            i = random.choice(range(self.cache_size))\n",
    "            self.cache = self.cache[:i] + [term] + self.cache[i+1:]\n",
    "        \n",
    "    def queue_ram(self, term): \n",
    "        if type(term) == list: \n",
    "            map(lambda x: self.queue_ram(x), term)\n",
    "            return\n",
    "        self.ram.append(term)\n",
    "            \n",
    "    def filter_by_pos(self, terms, pos): \n",
    "        return [x for x in terms if self.grammar.pos_dict[brown[x]] == pos]\n",
    "    \n",
    "    def gen_terms(self): \n",
    "        return self.embedding.get_word(self.embedding.filter_by_norm(self.rom*3 + self.ram + self.cache, range(1, len(self.rom*3 + self.ram + self.cache)+1)[::-1], prop = self.def_proportion))\n",
    "    \n",
    "    def get_pos(self, pos): \n",
    "        return self.filter_by_pos(self.gen_terms(), pos)\n",
    "            \n",
    "    def fill_characteristics(self, n = 4): \n",
    "        A = self.get_pos(\"<adjective>\")\n",
    "        random.shuffle(A)\n",
    "        A = A[:min([len(A), n])]\n",
    "        self.queue_ram(A)\n",
    "    \n",
    "    def pass_characteristics(self, C): \n",
    "        C.queue(self.rom + self.ram)\n",
    "    \n",
    "class Character(Wrapper): \n",
    "    def __init__(self, embedding, grammar, identity, name = None, pronoun = None, bases = None): \n",
    "        Wrapper.__init__(self, embedding, grammar)\n",
    "        self.identity = identity\n",
    "        self.rom = [identity]\n",
    "        self.pronoun = pronoun\n",
    "        self.name = name\n",
    "        if bases != None: \n",
    "            self.base(bases)\n",
    "        if self.pronoun == None: \n",
    "            self.pronoun = random.choice([\"he\", \"she\"])\n",
    "        if name == None: \n",
    "            if self.pronoun == \"he\": \n",
    "                self.name = random.choice(mnames)\n",
    "            else: \n",
    "                self.name = random.choice(fnames)\n",
    "\n",
    "    def __str__(self): \n",
    "        return self.name + \" is a \" + self.identity + \". \" + self.pronoun.capitalize() + \" is \" + \", \".join(self.ram[:-1]) + \", and \" + self.ram[-1] + \".\"\n",
    "        \n",
    "    def gen_char(self): \n",
    "        self.grammar.clear_terminals('<subject>')\n",
    "        self.grammar.clear_terminals('<pronoun>')\n",
    "        self.grammar.add_terminal(self.identity, '<subject>')\n",
    "        self.grammar.add_terminal_auto(self.identity)\n",
    "        self.grammar.add_terminal(self.pronoun, '<pronoun>')\n",
    "        return self.grammar.generate(attn_span = self.cache_size, variety = self.def_proportion, attn = self.ram)\n",
    "    \n",
    "    def profile(self, detail = 5): \n",
    "        k = self.name + \" is a \" + self.identity + \".\\n\"\n",
    "        for i, j in [(\"<verb>\", \"does\"), (\"<adjective>\", \"is\"), (\"<noun>\", \"likes\")]:\n",
    "            k += 'Some things ' + self.name + ' ' + j + ': \\n'\n",
    "            M = list(set(self.get_pos(i)))\n",
    "            random.shuffle(M)\n",
    "            if i == '<verb>': \n",
    "                k += '\\t' + ', '.join(map(lemma, M)[:min([detail, len(M)])]) + '\\n'\n",
    "            else: \n",
    "                k += '\\t' + ', '.join(M[:min([detail, len(M)])]) + '\\n'\n",
    "        return k\n",
    "    \n",
    "class Location(Wrapper): \n",
    "    def __init__(self, embedding, grammar, identity, name = None, bases = None): \n",
    "        Wrapper.__init__(self, embedding, grammar)\n",
    "        if name == None: \n",
    "            name = identity.capitalize()\n",
    "        self.identity = identity\n",
    "        self.name = name\n",
    "        self.rom = [identity]\n",
    "        if bases != None: \n",
    "            self.base(bases)\n",
    "        \n",
    "    def __str__(self): \n",
    "        return self.name + \" is a \" + self.identity + \". It is \" + \", \".join(self.ram[:-1]) + \", and \" + self.ram[-1] + \".\"\n",
    "    \n",
    "    def profile(self, detail = 5): \n",
    "        k = ''\n",
    "        for i, j in [(\"<verb>\", \"is used to do\"), (\"<adjective>\", \"is\"), (\"<noun>\", \"has\")]:\n",
    "            k += 'Some things ' + self.name + ' ' + j + ': \\n'\n",
    "            M = list(set(self.get_pos(i)))\n",
    "            random.shuffle(M)\n",
    "            if i == '<verb>': \n",
    "                k += '\\t' + ', '.join(map(lemma, M)[:min([detail, len(M)])]) + '\\n'\n",
    "            else: \n",
    "                k += '\\t' + ', '.join(M[:min([detail, len(M)])]) + '\\n'\n",
    "        return k\n",
    "    \n",
    "class Object(Wrapper): \n",
    "    def __init__(self, embedding, grammar, identity, name = None, bases = None): \n",
    "        Wrapper.__init__(self, embedding, grammar)\n",
    "        if name == None: \n",
    "            name = identity.capitalize()\n",
    "        self.identity = identity\n",
    "        self.name = name\n",
    "        self.rom = [identity]\n",
    "        if bases != None: \n",
    "            self.base(bases)\n",
    "    \n",
    "    def __str__(self): \n",
    "        return self.name + \" is a \" + self.identity + \". It is \" + \", \".join(self.ram[:-1]) + \", and \" + self.ram[-1] + \".\"\n",
    "    \n",
    "    def profile(self, detail = 5): \n",
    "        k = ''\n",
    "        for i, j in [(\"<verb>\", \"is used for\"), (\"<adjective>\", \"is\"), (\"<noun>\", \"is used with\")]:\n",
    "            k += 'Some things ' + self.name + ' ' + j + ': \\n'\n",
    "            M = list(set(self.get_pos(i)))\n",
    "            random.shuffle(M)\n",
    "            if i == '<verb>': \n",
    "                k += '\\t' + ', '.join(map(lemma, M)[:min([detail, len(M)])]) + '\\n'\n",
    "            else: \n",
    "                k += '\\t' + ', '.join(M[:min([detail, len(M)])]) + '\\n'\n",
    "        return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network: \n",
    "    def __init__(self, grammar, embedding, identity_list = {}): \n",
    "        self.identity_list = identity_list\n",
    "        self.grammar = grammar\n",
    "        self.embedding = embedding\n",
    "        self.entities = []\n",
    "        self.objects = []\n",
    "        \n",
    "    def make_canonical_char(self, identity, name = None, pronoun = None, adjs = []): \n",
    "        return Character(self.embedding, self.grammar, identity, name, pronoun, adjs)\n",
    "\n",
    "    def make_canonical_loc(self, identity, name = None, adjs = []): \n",
    "        return Location(self.embedding, self.grammar, identity, name, adjs)\n",
    "\n",
    "    def make_canonical_obj(self, identity, name = None, adjs = []): \n",
    "        return Object(self.embedding, self.grammar, identity, name, adjs)\n",
    "    \n",
    "    def add_char(self, identity, name, pronoun, adjs): \n",
    "        C = self.make_canonical_char(identity, name, pronoun, adjs)\n",
    "        C.fill_characteristics()\n",
    "        self.entities.append(C)\n",
    "        \n",
    "    def add_loc(self, identity, name, adjs): \n",
    "        C = self.make_canonical_loc(identity, name, adjs)\n",
    "        C.fill_characteristics()\n",
    "        self.entities.append(C)\n",
    "        \n",
    "    def add_obj(self, identity, name, adjs): \n",
    "        C = self.make_canonical_obj(identity, name, adjs)\n",
    "        C.fill_characteristics()\n",
    "        self.entities.append(C)\n",
    "\n",
    "    def get_next(self, first = None, force_loc = False): \n",
    "        if force_loc: \n",
    "            first = None\n",
    "        di = {}\n",
    "        if first == None or isinstance(first, Character): \n",
    "            di = self.identity_list\n",
    "        if isinstance(first, Location) or isinstance(first, Object): \n",
    "            di[\"characters\"] = self.identity_list[\"characters\"]\n",
    "        if force_loc: \n",
    "            di = {}\n",
    "            di[\"locations\"] = self.identity_list[\"locations\"]\n",
    "        I = random.choice(di.keys())\n",
    "        J = random.choice(di[I])\n",
    "\n",
    "        C = {\"locations\": self.make_canonical_loc, \"characters\": self.make_canonical_char, \"objects\": self.make_canonical_obj}[I](J)\n",
    "        if first != None: \n",
    "            first.increase_height()\n",
    "            first.pass_characteristics(C)\n",
    "            first.next.append(C)\n",
    "            C.previous = first\n",
    "        C.fill_characteristics()\n",
    "        if isinstance(C, Object) or isinstance(C, Location): \n",
    "            C.name = C.ram[0].capitalize() + \" \" + C.name\n",
    "        if isinstance(C, Character): \n",
    "            C.name = C.identity.capitalize() + \" \" + C.name \n",
    "        self.entities.append(C)\n",
    "        if isinstance(C, Object): \n",
    "            self.objects.append(C)\n",
    "        return C\n",
    "    \n",
    "    def make(self, n = 10): \n",
    "        t0 = time.time()\n",
    "        print \"The cast: \"\n",
    "        print \"\\t\" + self.get_next(force_loc = True).name + \" (1/\" + str(n) + \")\"\n",
    "        t = time.time()\n",
    "        A = [3, 3, 3, 3]\n",
    "        for i in range(n - 1): \n",
    "            P = sorted(self.entities, key = lambda x: len(x.next))[0]\n",
    "            t0 = t\n",
    "            t = time.time()\n",
    "            A = A[1:] + [t-t0]\n",
    "            m = sum(A)/float(len(A))\n",
    "            print \"\\t\" + self.get_next(random.choice(self.entities)).name + \" (\" + str(i+2) + \"/\" + str(n) + \", \" + str(round(m, 1) * (n - (i + 2))) + \"s remaining)\"\n",
    "        print 'Done!'\n",
    "    \n",
    "    def search(self, s): \n",
    "        for i in self.entities: \n",
    "            if s in i.name: \n",
    "                return i\n",
    "    \n",
    "    def quest(self, seed = None, deets = False): \n",
    "        if seed == None: \n",
    "            a = random.choice(sorted([x for x in self.entities if isinstance(x, Location)], key = lambda x: x.height, reverse = True)[:6])\n",
    "        else: \n",
    "            a = seed\n",
    "        L = [a] \n",
    "        while L[-1].next != []: \n",
    "            L.append(random.choice(sorted(L[-1].next, key = lambda x: x.height, reverse = True)[:min([len(L[-1].next), 2])]))\n",
    "        i = L[0]\n",
    "        print 'Your quest is: \\n'\n",
    "        if isinstance(i, Location): \n",
    "            print 'Go to the ' + i.name + '.'\n",
    "        if isinstance(i, Character): \n",
    "            print 'Find ' + i.name + '.'\n",
    "        if isinstance(i, Object): \n",
    "            print 'Retrieve the ' + i.name + '.'\n",
    "        if deets: \n",
    "            print \"\\t\" + i.__str__()\n",
    "        print \n",
    "        for i in L[1:-1]: \n",
    "            if isinstance(i, Location): \n",
    "                print 'Then, go to the ' + i.name + '.'\n",
    "            if isinstance(i, Character): \n",
    "                print 'Then, find ' + i.name + '.'\n",
    "            if isinstance(i, Object): \n",
    "                print 'Then, retrieve the ' + i.name + '.'\n",
    "            if deets: \n",
    "                print \"\\t\" + i.__str__()\n",
    "            print\n",
    "        i = L[-1]\n",
    "        if isinstance(i, Location): \n",
    "            print 'Finally, go to the ' + i.name + '.'\n",
    "        if isinstance(i, Character): \n",
    "            print 'Finally, find ' + i.name + '.'\n",
    "        if isinstance(i, Object): \n",
    "            print 'Finally, retrieve the ' + i.name + '.'\n",
    "        if deets: \n",
    "            print \"\\t\" + L[-1].__str__()\n",
    "            \n",
    "    def profile(self): \n",
    "        for i in self.entities: \n",
    "            print i\n",
    "            print '\\t' + ', '.join([x.name for x in i.next]) + '\\n'\n",
    "            \n",
    "    def char_profile(self, noun, detail = 8): \n",
    "        C = self.make_canonical_char(noun)\n",
    "        for i, j in [(\"<verb>\", \"do\"), (\"<adjective>\", \"are\"), (\"<noun>\", \"like\")]:\n",
    "            print 'Some things ' + noun + 's ' + j + ': '\n",
    "            M = list(set(C.get_pos(i)))\n",
    "            random.shuffle(M)\n",
    "            if i == '<verb>': \n",
    "                print '\\t' + ', '.join(map(lemma, M)[:min([detail, len(M)])])\n",
    "            else: \n",
    "                print '\\t' + ', '.join(M[:min([detail, len(M)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_productions = {'<sentence>': '<type1> <punctuation> <type2> <punctuation>|<type1> <punctuation> <type2> <comma> <connective> <type2> <punctuation>|<sentence> <sentence2>', \n",
    "                    '<sentence2>': '<type12> <punctuation> <type2> <punctuation>|<type12> <punctuation> <type2> <comma> <connective> <type2> <punctuation>',\n",
    "                    '<sentence3>': '<type12> <punctuation> <type2> <punctuation>|<type12> <punctuation> <type2> <comma> <connective> <type2> <punctuation>',\n",
    "                    '<type1>': '<subj> <description> <comma> <connective> <pronoun> <action>',\n",
    "                    '<type12>': '<subj2> <description> <comma> <connective> <pronoun> <action>',\n",
    "                    '<type2>': '<pronoun> <type3>|<pronoun> <causal> <type3>',\n",
    "                    '<type3>': '<description>|<action>',\n",
    "                    '<subj>': '<the> <subject>|<the> <adjective> <subject>|<the> <subject>',\n",
    "                    '<subj2>': '<the> <subject>',\n",
    "                    '<description>': '<be> <adjective>',\n",
    "                    '<action>': '<verb> <the> <adjective> <noun>'\n",
    "                   }\n",
    "\n",
    "char_terminals = {'<article>': 'a|the', \n",
    "                  '<the>': 'the', \n",
    "                 '<be>': 'became|was', \n",
    "                  '<noun>': '', \n",
    "                  '<adjective>': '', \n",
    "                  '<verb>': '', \n",
    "                  '<adverb>': '', \n",
    "                  '<pronoun>': '', \n",
    "                  '<prep>': 'with',\n",
    "                  '<subject>': '',\n",
    "                 '<connective>': 'as|and|while|but|so', \n",
    "                  '<causal>': 'then|therefore|accordingly',\n",
    "                 '<modifier>':'very|unusually|amazingly|unbelievably|somewhat|mildly|subtly',\n",
    "                 '<punctuation>': '.', \n",
    "                 '<comma>': ',', \n",
    "                 '<semicolon>': ';'\n",
    "                 }\n",
    "                    \n",
    "char_pos_dict = {'RB': '<adverb>', 'RBR': '<adjective>', 'RBS': '<adjective>', 'NN': '<noun>',\n",
    "                'NNS': '<noun>', 'JJ': '<adjective>', 'JJR': '<adjective>', 'JJS': '<adjective>', \n",
    "                'IN': '<prep>', 'VB': '<verb>', 'VBD': '<verb>', 'VBG': '<verb>', 'VBP': '<verb>', \n",
    "                 'VBZ': '<verb>', 'VBN': '<verb>', 'NNP': '<noun>', 'NNPS': '<noun>'}\n",
    "                    \n",
    "\n",
    "identity_list = {\"locations\": [\"house\", \"cave\", \"town\", \"university\", \"company\", \"district\", \"hall\", \"school\", \"route\", \"museum\", \n",
    "                              \"city\", \"church\", \"river\", \"island\", \"street\", \"court\", \"club\", \"mountain\", \"institute\", \"station\", \n",
    "                              \"province\", \"kingdom\", \"organization\", \"ship\", \"festival\", \"aircraft\", \"capital\", \"hill\", \"hospital\", \n",
    "                              \"studio\", \"municipality\", \"parliament\", \"soviet\", \"airport\", \"empire\", \"fort\", \"castle\", \"temple\", \n",
    "                              \"camp\", \"tower\", \"campus\", \"corporation\", \"prison\", \"senate\", \"farm\", \"cemetery\", \"institution\", \n",
    "                              \"federation\", \"dynasty\", \"commune\", \"cathedral\", \"junction\", \"exhibition\"],\n",
    "                 \"characters\": [\"cat\", \"dog\", \"person\", \"wizard\", \"professor\", \"soldier\", \"warrior\", \"president\", \"journalist\", \"priest\", \n",
    "                               \"chief\", \"singer\", \"saint\", \"emperor\", \"author\", \"leader\", \"student\", \"officer\", \"child\", \"author\",\n",
    "                               \"governor\", \"artist\", \"lord\", \"god\", \"captain\", \"writer\", \"manager\", \"actor\", \"doctor\", \"architect\", \n",
    "                               \"lieutenant\", \"representative\", \"communist\", \"spirit\", \"criminal\", \"poet\", \"colonel\", \"fighter\", \"lawyer\", \n",
    "                               \"historian\", \"guitarist\", \"wolf\", \"traitor\", \"villager\", \"farmer\", \"governor\"], \n",
    "                 \"objects\": [\"jewel\", \"tree\", \"flower\", \"book\", \"document\", \"computer\", \"weapon\", \"aircraft\", \"car\", \"painting\", \n",
    "                            \"note\", \"plant\", \"program\", \"stone\", \"recording\", \"journal\", \"guitar\", \"prize\", \"machine\", \"letter\", \"vehicle\", \n",
    "                            \"gun\", \"message\", \"crown\", \"drug\", \"map\", \"instrument\", \"element\", \"composition\"]\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get word vectors, stored in Pre, and cross-reference with Words and Words2\n",
    "\n",
    "Words2 = {x: None for x in open('/Users/6081iprep/Documents/languages/english.txt').readlines()}\n",
    "Pre = open('bow2.words','r').readlines()\n",
    "\n",
    "#Pre = open('/Users/6081iprep/Desktop/words.txt', 'r').read().split('\\n')[:-1]\n",
    "W = []\n",
    "V = []\n",
    "removewords = [\"blond\", \"virgin\", \"pregnant\", \"sucking\", \"dumb\", \"entitled\", \"titled\", \"smile\", \"cry\", \"gang\", \"dice\", \"black\", \"dick\", \"bye\", \n",
    "              \"byed\", \"byes\", \"gangs\", \"ganged\", \"spic\", \"beardless\", \"swart\", \"yeller\", \"mustachioed\", \"unshaven\", \"anchorite\", \"canonist\", \n",
    "              \"sodden\", \"stolid\", \"workmanlike\"] #these tend to create either inappropriate or nonsensical sentences\n",
    "\n",
    "for i in Pre: \n",
    "    if len(i) > 0: \n",
    "        w = i.split()[0]\n",
    "        if len(w) > 2:\n",
    "            try: \n",
    "                a = Words2[w + '\\n']\n",
    "                if w not in removewords: #lots of useless length 2 words\n",
    "                    W.append(w)\n",
    "                    V.append(map(float, i.split()[1:]))\n",
    "            except: \n",
    "                pass\n",
    "            \n",
    "WI = {}\n",
    "for i, w in enumerate(W): \n",
    "    WI[w] = i\n",
    "    \n",
    "del Words2 #save memory\n",
    "del Pre\n",
    "\n",
    "language='american'\n",
    "def load(lang):\n",
    "    a,n=open('/Users/6081iprep/Documents/names/'+lang+'.txt'),[]\n",
    "    for z in a:\n",
    "        n.append(z[:len(z)-1])\n",
    "    fnames,mnames=[],[]\n",
    "    for z in n:\n",
    "        q=z.split(',')\n",
    "        if q[2]=='female' or q[2]=='female\\r':\n",
    "            fnames.append(q[0])\n",
    "        else:\n",
    "            mnames.append(q[0])\n",
    "    a.close()\n",
    "    return fnames,mnames\n",
    "        \n",
    "fnames,mnames = load(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embedding\n",
      "Created grammar\n"
     ]
    }
   ],
   "source": [
    "ref_embedding = Embedding(V, WI)\n",
    "ref_embedding.strip_pos(perm = True)\n",
    "print(\"Created embedding\")\n",
    "#this will be the default Embedding instantiation for non-Embedding objects to turn to when they need to do stuff with word embeddings\n",
    "\n",
    "grammar = Grammar(char_productions, char_terminals, char_pos_dict)\n",
    "grammar.add_terminal_auto(ref_embedding.word2int.keys())\n",
    "basegrammar = Grammar(char_productions, char_terminals, char_pos_dict)\n",
    "basegrammar.add_terminal_auto(ref_embedding.word2int.keys())\n",
    "print(\"Created grammar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char = Character(ref_embedding, grammar, 'president')\n",
    "char.fill_characteristics()\n",
    "char.gen_char()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cast: \n",
      "\tElectoral Municipality (1/10)\n",
      "\tSaint Delores (2/10, 18.4s remaining)\n",
      "\tArtist Ethel (3/10, 17.5s remaining)\n",
      "\tWizard William (4/10, 16.2s remaining)\n",
      "\tGraphic Computer (5/10, 16.0s remaining)\n",
      "\tPoet Kerry (6/10, 17.2s remaining)\n",
      "\tFarmer Roberta (7/10, 13.8s remaining)\n",
      "\tWizard Tony (8/10, 9.2s remaining)\n",
      "\tDemoniac Journal (9/10, 4.4s remaining)\n",
      "\tCoppery Crown (10/10, 0.0s remaining)\n",
      "Done!\n",
      "Your quest is: \n",
      "\n",
      "Go to the Electoral Municipality.\n",
      "\tElectoral Municipality is a municipality. It is electoral, merrymaking, administrative, and populous.\n",
      "\n",
      "Then, find Artist Ethel.\n",
      "\tArtist Ethel is a artist. She is merrymaking, benighted, electoral, and demoniac.\n",
      "\n",
      "Finally, retrieve the Graphic Computer.\n",
      "\tGraphic Computer is a computer. It is graphic, phonic, merrymaking, and splashy.\n"
     ]
    }
   ],
   "source": [
    "N = Network(grammar, ref_embedding, identity_list)\n",
    "N.make(10)\n",
    "N.quest(deets = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
